{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3390,
     "status": "ok",
     "timestamp": 1653521449721,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "f1983b99",
    "outputId": "7fe4cf77-b06b-4e94-d89c-6716349a4c1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2, json, numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5199c1c3"
   },
   "source": [
    "### 路径说明\n",
    "```\n",
    "FS2K\n",
    "├─data # FS2K数据集位置\n",
    "│  └─FS2K\n",
    "│      ├─photo\n",
    "│      │  ├─photo1\n",
    "│      │  ├─photo2\n",
    "│      │  └─photo3\n",
    "│      └─sketch\n",
    "│          ├─sketch1\n",
    "│          ├─sketch2\n",
    "│          └─sketch3\n",
    "├─save # 模型保存位置\n",
    "└─FS2K.ipynb # 代码\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1574,
     "status": "ok",
     "timestamp": 1653521544055,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "bdac8441"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义Dataset\n",
    "class DS(Dataset):\n",
    "    def __init__(s, dataD, mode='train'):\n",
    "        super().__init__()\n",
    "        s.dataD = dataD\n",
    "        s.mode = mode\n",
    "        s.xtf = transforms.Compose([\n",
    "            transforms.Resize((250,250)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        s.ytf = transforms.Compose([\n",
    "            torch.tensor,\n",
    "        ])\n",
    "        s.data = s.read()\n",
    "    \n",
    "    def read(s):\n",
    "        D = s.dataD\n",
    "        jp = D / f'anno_{s.mode}.json'\n",
    "        with jp.open('r', encoding='utf-8')as f:\n",
    "            annos = json.load(f)\n",
    "        return annos\n",
    "\n",
    "    def __getitem__(s, i):\n",
    "        a = s.data[i]\n",
    "        imgP = s.dataD/ f\"photo/{a['image_name']}.jpg\"\n",
    "        img = s.xtf(Image.open(imgP.as_posix()))\n",
    "        colors = a['lip_color']+a['eye_color']\n",
    "        attrs = list(map(int,[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]))\n",
    "        return img, s.ytf(colors), torch.tensor(attrs, dtype=int)\n",
    "\n",
    "    def __len__(s):\n",
    "        return len(s.data)\n",
    "\n",
    "rootdir = '/content/drive/MyDrive/DeepLearningHW/'\n",
    "# 实例化Dataset\n",
    "dataD = Path(rootdir + 'data/FS2K')\n",
    "train_ds = DS(dataD)\n",
    "val_ds = DS(dataD, 'test')\n",
    "\n",
    "# 创建Dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaf59a07"
   },
   "source": [
    "### Dataset说明\n",
    "每一个样本包含三个变量img, colors, attrs  \n",
    "img为tensor图片  \n",
    "colors为一个6元素的float类型一维数组, 前三个表示嘴唇颜色lip_color, 后三个表示眼睛颜色eye_color  \n",
    "attrs为6元素的整型一位数组, 分别为hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d058427f",
    "outputId": "fabb2c80-7bc0-4c7f-a3ba-21be81680ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([156.9775,  82.5112,  79.0000, 118.6518,  72.2589,  69.5982]),\n",
       " tensor([0, 2, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, colors, attrs = train_ds[0]\n",
    "colors, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb335b7f",
    "outputId": "62354c27-3732-4d18-c8f3-72a3bef8732a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'photo1/image0110',\n",
       " 'skin_patch': [163, 139],\n",
       " 'lip_color': [156.97750511247443, 82.51124744376278, 79.0],\n",
       " 'eye_color': [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
       " 'hair': 0,\n",
       " 'hair_color': 2,\n",
       " 'gender': 0,\n",
       " 'earring': 1,\n",
       " 'smile': 1,\n",
       " 'frontal_face': 1,\n",
       " 'style': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"image_name\": \"photo1/image0110\",\n",
    "\n",
    "    \"skin_patch\": [163, 139],\n",
    "    # a point of face region.\n",
    "\n",
    "    \"lip_color\": [156.97750511247443, 82.51124744376278, 79.0],\n",
    "    # the mean RGB value of lip area.\n",
    "\n",
    "    \"eye_color\": [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
    "    # the mean RGB value of eye area.\n",
    "\n",
    "    \"hair\": 0,\n",
    "    # 0: with hair, 1: without hair.\n",
    "\n",
    "    \"hair_color\": 2,\n",
    "    # 0: brown, 1: black, 2: red, 3: no-hair, 4: golden.\n",
    "\n",
    "    \"gender\": 0,\n",
    "    # 0: male, 1: female.\n",
    "\n",
    "    \"earring\": 1,\n",
    "    # 0: with earring, 1: without earring.\n",
    "\n",
    "    \"smile\": 1,\n",
    "    # 0: with smile, 1: without smile.\n",
    "\n",
    "    \"frontal_face\": 1,\n",
    "    # 0: head rotates within 30 degrees, 1: > 30 degrees\n",
    "\n",
    "    \"style\": 0\n",
    "    # Style = one of {0, 1, 2}, please refer to the sketch samples.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1653521453370,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "41599ff2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, \n",
    "            padding=1, bias=True, groups=1):    \n",
    "    \"\"\"3x3 convolution with padding\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3, \n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        bias=bias,\n",
    "        groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    \"\"\"1x1 convolution with padding\n",
    "    - Normal pointwise convolution When groups == 1\n",
    "    - Grouped pointwise convolution when groups > 1\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=1, \n",
    "        groups=groups,\n",
    "        stride=1)\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "\n",
    "    channels_per_group = num_channels // groups\n",
    "    \n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups, \n",
    "        channels_per_group, height, width)\n",
    "\n",
    "    # transpose\n",
    "    # - contiguous() required if transpose() is used before view().\n",
    "    #   See https://github.com/pytorch/pytorch/issues/764\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ShuffleUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, groups=3,\n",
    "                 grouped_conv=True, combine='add'):\n",
    "        \n",
    "        super(ShuffleUnit, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.grouped_conv = grouped_conv\n",
    "        self.combine = combine\n",
    "        self.groups = groups\n",
    "        self.bottleneck_channels = self.out_channels // 4\n",
    "\n",
    "        # define the type of ShuffleUnit\n",
    "        if self.combine == 'add':\n",
    "            # ShuffleUnit Figure 2b\n",
    "            self.depthwise_stride = 1\n",
    "            self._combine_func = self._add\n",
    "        elif self.combine == 'concat':\n",
    "            # ShuffleUnit Figure 2c\n",
    "            self.depthwise_stride = 2\n",
    "            self._combine_func = self._concat\n",
    "            \n",
    "            # ensure output of concat has the same channels as \n",
    "            # original output channels.\n",
    "            self.out_channels -= self.in_channels\n",
    "        else:\n",
    "            raise ValueError(\"Cannot combine tensors with \\\"{}\\\"\" \\\n",
    "                             \"Only \\\"add\\\" and \\\"concat\\\" are\" \\\n",
    "                             \"supported\".format(self.combine))\n",
    "\n",
    "        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n",
    "        # to bottleneck channels, as in a ResNet bottleneck module.\n",
    "        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n",
    "        self.first_1x1_groups = self.groups if grouped_conv else 1\n",
    "\n",
    "        self.g_conv_1x1_compress = self._make_grouped_conv1x1(\n",
    "            self.in_channels,\n",
    "            self.bottleneck_channels,\n",
    "            self.first_1x1_groups,\n",
    "            batch_norm=True,\n",
    "            relu=True\n",
    "            )\n",
    "\n",
    "        # 3x3 depthwise convolution followed by batch normalization\n",
    "        self.depthwise_conv3x3 = conv3x3(\n",
    "            self.bottleneck_channels, self.bottleneck_channels,\n",
    "            stride=self.depthwise_stride, groups=self.bottleneck_channels)\n",
    "        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n",
    "\n",
    "        # Use 1x1 grouped convolution to expand from \n",
    "        # bottleneck_channels to out_channels\n",
    "        self.g_conv_1x1_expand = self._make_grouped_conv1x1(\n",
    "            self.bottleneck_channels,\n",
    "            self.out_channels,\n",
    "            self.groups,\n",
    "            batch_norm=True,\n",
    "            relu=False\n",
    "            )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _add(x, out):\n",
    "        # residual connection\n",
    "        return x + out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, out):\n",
    "        # concatenate along channel axis\n",
    "        return torch.cat((x, out), 1)\n",
    "\n",
    "\n",
    "    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n",
    "        batch_norm=True, relu=False):\n",
    "\n",
    "        modules = OrderedDict()\n",
    "\n",
    "        conv = conv1x1(in_channels, out_channels, groups=groups)\n",
    "        modules['conv1x1'] = conv\n",
    "\n",
    "        if batch_norm:\n",
    "            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n",
    "        if relu:\n",
    "            modules['relu'] = nn.ReLU()\n",
    "        if len(modules) > 1:\n",
    "            return nn.Sequential(modules)\n",
    "        else:\n",
    "            return conv\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # save for combining later with output\n",
    "        residual = x\n",
    "\n",
    "        if self.combine == 'concat':\n",
    "            residual = F.avg_pool2d(residual, kernel_size=3, \n",
    "                stride=2, padding=1)\n",
    "\n",
    "        out = self.g_conv_1x1_compress(x)\n",
    "        out = channel_shuffle(out, self.groups)\n",
    "        out = self.depthwise_conv3x3(out)\n",
    "        out = self.bn_after_depthwise(out)\n",
    "        out = self.g_conv_1x1_expand(out)\n",
    "        \n",
    "        out = self._combine_func(residual, out)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ShuffleNet(nn.Module):\n",
    "    \"\"\"ShuffleNet implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, groups=3, in_channels=3, num_classes=1000):\n",
    "        \"\"\"ShuffleNet constructor.\n",
    "\n",
    "        Arguments:\n",
    "            groups (int, optional): number of groups to be used in grouped \n",
    "                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n",
    "                performance according to original paper.\n",
    "            in_channels (int, optional): number of channels in the input tensor.\n",
    "                Default is 3 for RGB image inputs.\n",
    "            num_classes (int, optional): number of classes to predict. Default\n",
    "                is 1000 for ImageNet.\n",
    "\n",
    "        \"\"\"\n",
    "        super(ShuffleNet, self).__init__()\n",
    "\n",
    "        self.groups = groups\n",
    "        self.stage_repeats = [3, 7, 3]\n",
    "        self.in_channels =  in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # index 0 is invalid and should never be called.\n",
    "        # only used for indexing convenience.\n",
    "        if groups == 1:\n",
    "            self.stage_out_channels = [-1, 24, 144, 288, 567]\n",
    "        elif groups == 2:\n",
    "            self.stage_out_channels = [-1, 24, 200, 400, 800]\n",
    "        elif groups == 3:\n",
    "            self.stage_out_channels = [-1, 24, 240, 480, 960]\n",
    "        elif groups == 4:\n",
    "            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n",
    "        elif groups == 8:\n",
    "            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"{} groups is not supported for\n",
    "                   1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "        \n",
    "        # Stage 1 always has 24 output channels\n",
    "        self.conv1 = conv3x3(self.in_channels,\n",
    "                             self.stage_out_channels[1], # stage 1\n",
    "                             stride=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = self._make_stage(2)\n",
    "        # Stage 3\n",
    "        self.stage3 = self._make_stage(3)\n",
    "        # Stage 4\n",
    "        self.stage4 = self._make_stage(4)\n",
    "\n",
    "        # Global pooling:\n",
    "        # Undefined as PyTorch's functional API can be used for on-the-fly\n",
    "        # shape inference if input size is not ImageNet's 224x224\n",
    "\n",
    "        # Fully-connected classification layer\n",
    "        num_inputs = self.stage_out_channels[-1]\n",
    "\n",
    "        self.lip_color = nn.Linear(num_inputs, 3)\n",
    "        self.eye_color = nn.Linear(num_inputs, 3)\n",
    "        self.hair = nn.Linear(num_inputs, 2)\n",
    "        self.hair_color = nn.Linear(num_inputs, 5)\n",
    "        self.gender = nn.Linear(num_inputs, 2)\n",
    "        self.earring = nn.Linear(num_inputs, 2)\n",
    "        self.smile = nn.Linear(num_inputs, 2)\n",
    "        self.frontal_face = nn.Linear(num_inputs, 2)\n",
    "\n",
    "        # self.fc = nn.Linear(num_inputs, self.num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_stage(self, stage):\n",
    "        modules = OrderedDict()\n",
    "        stage_name = \"ShuffleUnit_Stage{}\".format(stage)\n",
    "        \n",
    "        # First ShuffleUnit in the stage\n",
    "        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n",
    "        #   is used in Stage 2. Group convolutions used everywhere else.\n",
    "        grouped_conv = stage > 2\n",
    "        \n",
    "        # 2. concatenation unit is always used.\n",
    "        first_module = ShuffleUnit(\n",
    "            self.stage_out_channels[stage-1],\n",
    "            self.stage_out_channels[stage],\n",
    "            groups=self.groups,\n",
    "            grouped_conv=grouped_conv,\n",
    "            combine='concat'\n",
    "            )\n",
    "        modules[stage_name+\"_0\"] = first_module\n",
    "\n",
    "        # add more ShuffleUnits depending on pre-defined number of repeats\n",
    "        for i in range(self.stage_repeats[stage-2]):\n",
    "            name = stage_name + \"_{}\".format(i+1)\n",
    "            module = ShuffleUnit(\n",
    "                self.stage_out_channels[stage],\n",
    "                self.stage_out_channels[stage],\n",
    "                groups=self.groups,\n",
    "                grouped_conv=True,\n",
    "                combine='add'\n",
    "                )\n",
    "            modules[name] = module\n",
    "\n",
    "        return nn.Sequential(modules)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        # global average pooling layer\n",
    "        x = F.avg_pool2d(x, x.data.size()[-2:])\n",
    "        \n",
    "        # flatten for input to fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        lip_color = self.lip_color(x)\n",
    "        eye_color = self.eye_color(x)\n",
    "        hair = self.hair(x)\n",
    "        hair_color = self.hair_color(x)\n",
    "        gender = self.gender(x)\n",
    "        earring = self.earring(x)\n",
    "        smile = self.smile(x)\n",
    "        frontal_face = self.frontal_face(x)\n",
    "\n",
    "        hair = F.softmax(hair,dim = 1)\n",
    "        hair_color = F.softmax(hair_color,dim = 1)\n",
    "        gender = F.softmax(gender,dim = 1)\n",
    "        earring = F.softmax(earring,dim = 1)\n",
    "        smile = F.softmax(smile,dim = 1)\n",
    "        frontal_face = F.softmax(frontal_face,dim = 1)\n",
    "\n",
    "\n",
    "        return [lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face]\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(s):\n",
    "        super().__init__()\n",
    "        s.MSE = torch.nn.MSELoss()\n",
    "        s.CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(s, preds, colors_b, attrs_b):\n",
    "#         y = a['lip_color']+a['eye_color']+[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]\n",
    "        lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = preds\n",
    "        lpc = s.MSE(lip_color, colors_b[:,:3])\n",
    "        lc = s.MSE(eye_color, colors_b[:,3:])\n",
    "        h = s.CE(hair, attrs_b[:, 0])\n",
    "        hc = s.CE(hair_color, attrs_b[:, 1])\n",
    "        g = s.CE(gender, attrs_b[:, 2])\n",
    "        e = s.CE(earring, attrs_b[:, 3])\n",
    "        sm = s.CE(smile, attrs_b[:, 4])\n",
    "        f = s.CE(frontal_face, attrs_b[:, 5])\n",
    "        loss = lpc+lc+h+hc+g+e+sm+f\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8e22db1"
   },
   "source": [
    "### 模型和损失函数说明\n",
    "模型使用Mobilenetv3的Small版本， 在将最后的输出层更改为8个并行的1x1卷积层，对2个颜色属性进行回归，对6个整型属性进行分类  \n",
    "回归损失使用MSE，分类损失使用交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a740a7b",
    "outputId": "28b1a3a2-3fbc-4c9a-f6d6-b3ae1513e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8925,  0.1118, -0.1723],\n",
       "         [-0.1632,  0.2536, -1.3730]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.0142, -0.2719,  0.8314],\n",
       "         [-2.5911, -0.9223, -0.0742]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.8087, -1.0785],\n",
       "         [ 0.4037,  1.3462]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.9686, -1.5095,  0.3642, -0.6491, -0.6247],\n",
       "         [-2.1093, -0.1983, -0.2731, -0.8785, -1.9321]], grad_fn=<ViewBackward>),\n",
       " tensor([[-2.1952,  0.4042],\n",
       "         [-2.5527, -0.1194]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.1786, -2.2074],\n",
       "         [ 0.2929, -1.5629]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.4470,  0.1229],\n",
       "         [-0.4061,  0.9815]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.8123, -1.0342],\n",
       "         [ 1.5224, -1.1291]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = mobilenet_v3_small_1_0(nclass=6)\n",
    "x = torch.rand((2, 3, 250, 250))\n",
    "y = m(x)\n",
    "for _ in y:\n",
    "    print(_.shape)\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = y\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653521460999,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "ae478728"
   },
   "outputs": [],
   "source": [
    "def save(savePath, m, epoch, acc):\n",
    "    d = {\n",
    "        'param': m.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'acc': acc,\n",
    "    }\n",
    "    if isinstance(savePath, Path):\n",
    "        savePath = savePath.as_posix()\n",
    "    torch.save(d, savePath)\n",
    "    print('checkpoint saved as', savePath)\n",
    "\n",
    "def load(loadPath):\n",
    "    if isinstance(loadPath, Path):\n",
    "        loadPath = loadPath.as_posix()\n",
    "    d = torch.load(loadPath)\n",
    "    m = Model()\n",
    "    m.load_state_dict(d['param'])\n",
    "    print('checkpoint loaded from', loadPath)\n",
    "    e, acc = d['epoch'], d['acc']\n",
    "    print('epoch:', e, 'acc:', acc)\n",
    "    return m, d['epoch'], d['acc']\n",
    "\n",
    "def toCpu(path):\n",
    "    path = Path(path)\n",
    "    m, e, acc = load(path)\n",
    "    m.to(torch.device('cpu'))\n",
    "    save(path.parents[0]/f'{path.stem}_cpu.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1653521462900,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "9081a11d"
   },
   "outputs": [],
   "source": [
    "def val(em, param, val_dl, d):\n",
    "    l=Loss()\n",
    "    vn = len(val_dl.dataset)\n",
    "    em.load_state_dict(param)\n",
    "    hair_cnt = 0\n",
    "    hair_color_cnt = 0\n",
    "    gender_cnt = 0\n",
    "    earring_cnt = 0\n",
    "    smile_cnt = 0\n",
    "    frontal_face_cnt = 0\n",
    "    L = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(val_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            outs = em(xs)\n",
    "            lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = outs\n",
    "            L += l(outs, colors_b, attrs_b.long()).item()\n",
    "            hair_ = torch.max(hair, 1)[1]\n",
    "            hair_cnt += torch.sum(hair_ == attrs_b[:,0])\n",
    "            \n",
    "            hair_color_ = torch.max(hair_color, 1)[1]\n",
    "            hair_color_cnt += torch.sum(hair_color_ == attrs_b[:,1])\n",
    "            \n",
    "            gender_ = torch.max(gender, 1)[1]\n",
    "            gender_cnt += torch.sum(gender_ == attrs_b[:,2])\n",
    "            \n",
    "            earring_ = torch.max(earring, 1)[1]\n",
    "            earring_cnt += torch.sum(earring_ == attrs_b[:,3])\n",
    "            \n",
    "            smile_ = torch.max(smile, 1)[1]\n",
    "            smile_cnt += torch.sum(smile_ == attrs_b[:,4])\n",
    "            \n",
    "            frontal_face_ = torch.max(frontal_face, 1)[1]\n",
    "            frontal_face_cnt += torch.sum(frontal_face_ == attrs_b[:,5])\n",
    "    acc = (hair_cnt+hair_color_cnt+gender_cnt+earring_cnt+smile_cnt+frontal_face_cnt)/6/vn\n",
    "    print(f'validated on {vn} samples| mean acc:{acc*100:.4f}%')\n",
    "    print(f'hair_cnt:{hair_cnt/vn}|hair_color_cnt:{hair_color_cnt/vn}|gender_cnt:{gender_cnt/vn}')\n",
    "    print(f'earring_cnt:{earring_cnt/vn}|smile_cnt:{smile_cnt/vn}|frontal_face_cnt:{frontal_face_cnt/vn}')\n",
    "    print(f'loss:{L/vn:.4f}')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train(m,\n",
    "          d,\n",
    "          train_dl,\n",
    "          val_dl,\n",
    "          saveDir=Path('save'),\n",
    "          resumePath=None,\n",
    "          lr=0.001,\n",
    "          e=50,\n",
    "          s=10\n",
    "         ):\n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    startEp = -1\n",
    "    b = 0\n",
    "    try:\n",
    "        m, startEp, b = load(saveDir/'best.ckpt')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    if resumePath is not None:\n",
    "        m, startEp, b = load(resumePath)\n",
    "\n",
    "    m.to(d).train()\n",
    "    em = Model().to(d).eval()\n",
    "    \n",
    "    l=Loss()\n",
    "    o=torch.optim.SGD(m.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    tn = len(train_dl.dataset)\n",
    "    t = tqdm(range(startEp+1, e))\n",
    "#     t = range(startEp+1, e)\n",
    "    for ep in t:\n",
    "        L = 0\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(train_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            o.zero_grad()\n",
    "            outs = m(xs)\n",
    "            loss = l(outs, colors_b, attrs_b.long())\n",
    "            loss.backward()\n",
    "            o.step()\n",
    "\n",
    "            L += loss.item()\n",
    "        t.set_description(f'ep:{ep}| L:{L/tn:.6f}')\n",
    "        if (ep+1)%s != 0: continue\n",
    "\n",
    "        acc = val(em, m.state_dict(), val_dl, d)\n",
    "        save(saveDir/f'{ep:05d}_{acc:.4f}.ckpt', m, ep, acc)\n",
    "        if b < acc:\n",
    "            b = acc\n",
    "            save(saveDir/'best.ckpt', m, ep, acc)\n",
    "        print(f'E:{ep}| L:{L/tn:.6f}')\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 796547,
     "status": "ok",
     "timestamp": 1653522347914,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "e6e5fcfb",
    "outputId": "28102a24-faad-40cd-bf10-82d2eebc5145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "checkpoint loaded from /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "epoch: 379 acc: tensor(0.7702, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:399| L:1.123851: 100%|██████████| 20/20 [13:13<00:00, 39.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:77.1192%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.5124282836914062|gender_cnt:0.8470363020896912\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6625239253044128|frontal_face_cnt:0.8336520195007324\n",
      "loss:27.1113\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00399_0.7712.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:399| L:1.123851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(d)\n",
    "# d = torch.device('cpu')\n",
    "Model = ShuffleNet\n",
    "m = Model().to(d)\n",
    "train(m,\n",
    "      d,\n",
    "      train_dl,\n",
    "      val_dl,\n",
    "      saveDir=Path(rootdir + 'save2'),\n",
    "#       resumePath=Path('save2/03999_0.8062.ckpt'),\n",
    "      lr=0.0001,\n",
    "      e=400,\n",
    "      s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5709,
     "status": "ok",
     "timestamp": 1653522381905,
     "user": {
      "displayName": "JL D",
      "userId": "04410234866674325568"
     },
     "user_tz": -480
    },
    "id": "06aa49de",
    "outputId": "7ea2410d-ede4-4818-85aa-cf9cef651f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded from /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "epoch: 399 acc: tensor(0.7712, device='cuda:0')\n",
      "validated on 1046 samples| mean acc:77.1192%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.5124282836914062|gender_cnt:0.8470363020896912\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6625239253044128|frontal_face_cnt:0.8336520195007324\n",
      "loss:27.0248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7712, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em, epo, acc = load(rootdir + 'save2/best.ckpt')\n",
    "val(Model().to(d).eval(), em.state_dict(), val_dl, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7e15841"
   },
   "source": [
    "## result\n",
    "\n",
    "epoch: 399 acc: tensor(0.7712, device='cuda:0')  \n",
    "validated on 1046 samples  \n",
    "mean acc:77.1192%    \n",
    "hair_cnt:0.9502868056297302  \n",
    "hair_color_cnt:0.5124282836914062  \n",
    "gender_cnt:0.8470363020896912  \n",
    "earring_cnt:0.8212236762046814  \n",
    "smile_cnt:0.6625239253044128  \n",
    "frontal_face_cnt:0.8336520195007324  \n",
    "loss:27.0248  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
