{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1653476251619,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "f1983b99",
    "outputId": "72c63301-a80a-4fd9-d902-44f645ff2eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2, json, numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5199c1c3"
   },
   "source": [
    "### 路径说明\n",
    "```\n",
    "FS2K\n",
    "├─data # FS2K数据集位置\n",
    "│  └─FS2K\n",
    "│      ├─photo\n",
    "│      │  ├─photo1\n",
    "│      │  ├─photo2\n",
    "│      │  └─photo3\n",
    "│      └─sketch\n",
    "│          ├─sketch1\n",
    "│          ├─sketch2\n",
    "│          └─sketch3\n",
    "├─save # 模型保存位置\n",
    "└─FS2K.ipynb # 代码\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1653476252735,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "bdac8441"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义Dataset\n",
    "class DS(Dataset):\n",
    "    def __init__(s, dataD, mode='train'):\n",
    "        super().__init__()\n",
    "        s.dataD = dataD\n",
    "        s.mode = mode\n",
    "        s.xtf = transforms.Compose([\n",
    "            transforms.Resize((250,250)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        s.ytf = transforms.Compose([\n",
    "            torch.tensor,\n",
    "        ])\n",
    "        s.data = s.read()\n",
    "    \n",
    "    def read(s):\n",
    "        D = s.dataD\n",
    "        jp = D / f'anno_{s.mode}.json'\n",
    "        with jp.open('r', encoding='utf-8')as f:\n",
    "            annos = json.load(f)\n",
    "        return annos\n",
    "\n",
    "    def __getitem__(s, i):\n",
    "        a = s.data[i]\n",
    "        imgP = s.dataD/ f\"photo/{a['image_name']}.jpg\"\n",
    "        img = s.xtf(Image.open(imgP.as_posix()))\n",
    "        colors = a['lip_color']+a['eye_color']\n",
    "        attrs = list(map(int,[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]))\n",
    "        return img, s.ytf(colors), torch.tensor(attrs, dtype=int)\n",
    "\n",
    "    def __len__(s):\n",
    "        return len(s.data)\n",
    "\n",
    "rootdir = './DeepLearningHW/'\n",
    "# 实例化Dataset\n",
    "dataD = Path(rootdir + 'data/FS2K')\n",
    "train_ds = DS(dataD)\n",
    "val_ds = DS(dataD, 'test')\n",
    "\n",
    "# 创建Dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaf59a07"
   },
   "source": [
    "### Dataset说明\n",
    "每一个样本包含三个变量img, colors, attrs  \n",
    "img为tensor图片  \n",
    "colors为一个6元素的float类型一维数组, 前三个表示嘴唇颜色lip_color, 后三个表示眼睛颜色eye_color  \n",
    "attrs为6元素的整型一位数组, 分别为hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d058427f",
    "outputId": "fabb2c80-7bc0-4c7f-a3ba-21be81680ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([156.9775,  82.5112,  79.0000, 118.6518,  72.2589,  69.5982]),\n",
       " tensor([0, 2, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, colors, attrs = train_ds[0]\n",
    "colors, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb335b7f",
    "outputId": "62354c27-3732-4d18-c8f3-72a3bef8732a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'photo1/image0110',\n",
       " 'skin_patch': [163, 139],\n",
       " 'lip_color': [156.97750511247443, 82.51124744376278, 79.0],\n",
       " 'eye_color': [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
       " 'hair': 0,\n",
       " 'hair_color': 2,\n",
       " 'gender': 0,\n",
       " 'earring': 1,\n",
       " 'smile': 1,\n",
       " 'frontal_face': 1,\n",
       " 'style': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"image_name\": \"photo1/image0110\",\n",
    "\n",
    "    \"skin_patch\": [163, 139],\n",
    "    # a point of face region.\n",
    "\n",
    "    \"lip_color\": [156.97750511247443, 82.51124744376278, 79.0],\n",
    "    # the mean RGB value of lip area.\n",
    "\n",
    "    \"eye_color\": [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
    "    # the mean RGB value of eye area.\n",
    "\n",
    "    \"hair\": 0,\n",
    "    # 0: with hair, 1: without hair.\n",
    "\n",
    "    \"hair_color\": 2,\n",
    "    # 0: brown, 1: black, 2: red, 3: no-hair, 4: golden.\n",
    "\n",
    "    \"gender\": 0,\n",
    "    # 0: male, 1: female.\n",
    "\n",
    "    \"earring\": 1,\n",
    "    # 0: with earring, 1: without earring.\n",
    "\n",
    "    \"smile\": 1,\n",
    "    # 0: with smile, 1: without smile.\n",
    "\n",
    "    \"frontal_face\": 1,\n",
    "    # 0: head rotates within 30 degrees, 1: > 30 degrees\n",
    "\n",
    "    \"style\": 0\n",
    "    # Style = one of {0, 1, 2}, please refer to the sketch samples.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1653476702365,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "9561edc6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class _Hswish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(_Hswish, self).__init__()\n",
    "        self.relu6 = nn.ReLU6(inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class _Hsigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(_Hsigmoid, self).__init__()\n",
    "        self.relu6 = nn.ReLU6(inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class _ConvBNHswish(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(_ConvBNHswish, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = _Hswish(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            _Hsigmoid(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, _, _ = x.size()\n",
    "        out = self.avg_pool(x).view(n, c)\n",
    "        out = self.fc(out).view(n, c, 1, 1)\n",
    "        return x * out.expand_as(x)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, exp_size, kernel_size, stride, dilation=1, se=False, nl='RE',\n",
    "                 norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "        if nl == 'HS':\n",
    "            act = _Hswish\n",
    "        else:\n",
    "            act = nn.ReLU\n",
    "        if se:\n",
    "            SELayer = SEModule\n",
    "        else:\n",
    "            SELayer = Identity\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            nn.Conv2d(in_channels, exp_size, 1, bias=False),\n",
    "            norm_layer(exp_size),\n",
    "            act(True),\n",
    "            # dw\n",
    "            nn.Conv2d(exp_size, exp_size, kernel_size, stride, (kernel_size - 1) // 2 * dilation,\n",
    "                      dilation, groups=exp_size, bias=False),\n",
    "            norm_layer(exp_size),\n",
    "            SELayer(exp_size),\n",
    "            act(True),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(exp_size, out_channels, 1, bias=False),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, nclass=1000, mode='large', width_mult=1.0, dilated=False, norm_layer=nn.BatchNorm2d):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        if mode == 'large':\n",
    "            layer1_setting = [\n",
    "                # k, exp_size, c, se, nl, s\n",
    "                [3, 16, 16, False, 'RE', 1],\n",
    "                [3, 64, 24, False, 'RE', 2],\n",
    "                [3, 72, 24, False, 'RE', 1], ]\n",
    "            layer2_setting = [\n",
    "                [5, 72, 40, True, 'RE', 2],\n",
    "                [5, 120, 40, True, 'RE', 1],\n",
    "                [5, 120, 40, True, 'RE', 1], ]\n",
    "            layer3_setting = [\n",
    "                [3, 240, 80, False, 'HS', 2],\n",
    "                [3, 200, 80, False, 'HS', 1],\n",
    "                [3, 184, 80, False, 'HS', 1],\n",
    "                [3, 184, 80, False, 'HS', 1],\n",
    "                [3, 480, 112, True, 'HS', 1],\n",
    "                [3, 672, 112, True, 'HS', 1],\n",
    "                [5, 672, 112, True, 'HS', 1], ]\n",
    "            layer4_setting = [\n",
    "                [5, 672, 160, True, 'HS', 2],\n",
    "                [5, 960, 160, True, 'HS', 1], ]\n",
    "        elif mode == 'small':\n",
    "            layer1_setting = [\n",
    "                # k, exp_size, c, se, nl, s\n",
    "                [3, 16, 16, True, 'RE', 2], ]\n",
    "            layer2_setting = [\n",
    "                [3, 72, 24, False, 'RE', 2],\n",
    "                [3, 88, 24, False, 'RE', 1], ]\n",
    "            layer3_setting = [\n",
    "                [5, 96, 40, True, 'HS', 2],\n",
    "                [5, 240, 40, True, 'HS', 1],\n",
    "                [5, 240, 40, True, 'HS', 1],\n",
    "                [5, 120, 48, True, 'HS', 1],\n",
    "                [5, 144, 48, True, 'HS', 1], ]\n",
    "            layer4_setting = [\n",
    "                [5, 288, 96, True, 'HS', 2],\n",
    "                [5, 576, 96, True, 'HS', 1],\n",
    "                [5, 576, 96, True, 'HS', 1], ]\n",
    "        else:\n",
    "            raise ValueError('Unknown mode.')\n",
    "\n",
    "        # building first layer\n",
    "        self.in_channels = int(16 * width_mult) if width_mult > 1.0 else 16\n",
    "        self.conv1 = _ConvBNHswish(3, self.in_channels, 3, 2, 1, norm_layer=norm_layer)\n",
    "\n",
    "        # building bottleneck blocks\n",
    "        self.layer1 = self._make_layer(Bottleneck, layer1_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        self.layer2 = self._make_layer(Bottleneck, layer2_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        self.layer3 = self._make_layer(Bottleneck, layer3_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        if dilated:\n",
    "            self.layer4 = self._make_layer(Bottleneck, layer4_setting,\n",
    "                                           width_mult, dilation=2, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.layer4 = self._make_layer(Bottleneck, layer4_setting,\n",
    "                                           width_mult, norm_layer=norm_layer)\n",
    "\n",
    "        # building last several layers\n",
    "        classifier = list()\n",
    "        if mode == 'large':\n",
    "            last_bneck_channels = int(960 * width_mult) if width_mult > 1.0 else 960\n",
    "            self.layer5 = _ConvBNHswish(self.in_channels, last_bneck_channels, 1, norm_layer=norm_layer)\n",
    "            classifier.append(nn.AdaptiveAvgPool2d(1))\n",
    "            classifier.append(nn.Conv2d(last_bneck_channels, 1280, 1))\n",
    "            classifier.append(_Hswish(True))\n",
    "#             classifier.append(nn.Conv2d(1280, nclass, 1))\n",
    "        elif mode == 'small':\n",
    "            last_bneck_channels = int(576 * width_mult) if width_mult > 1.0 else 576\n",
    "            self.layer5 = _ConvBNHswish(self.in_channels, last_bneck_channels, 1, norm_layer=norm_layer)\n",
    "            classifier.append(SEModule(last_bneck_channels))\n",
    "            classifier.append(nn.AdaptiveAvgPool2d(1))\n",
    "            classifier.append(nn.Conv2d(last_bneck_channels, 1280, 1))\n",
    "            classifier.append(_Hswish(True))\n",
    "#             classifier.append(nn.Conv2d(1280, nclass, 1))\n",
    "        else:\n",
    "            raise ValueError('Unknown mode.')\n",
    "#         self.classifier = nn.Sequential(*classifier)\n",
    "\n",
    "        self.flat1280 = nn.Sequential(*classifier)\n",
    "        n = 1280\n",
    "        self.lip_color = nn.Conv2d(n, 3, 1)\n",
    "        self.eye_color = nn.Conv2d(n, 3, 1)\n",
    "        self.hair = nn.Conv2d(n, 2, 1)\n",
    "        self.hair_color = nn.Conv2d(n, 5, 1)\n",
    "        self.gender = nn.Conv2d(n, 2, 1)\n",
    "        self.earring = nn.Conv2d(n, 2, 1)\n",
    "        self.smile = nn.Conv2d(n, 2, 1)\n",
    "        self.frontal_face = nn.Conv2d(n, 2, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, block_setting, width_mult, dilation=1, norm_layer=nn.BatchNorm2d):\n",
    "        layers = list()\n",
    "        for k, exp_size, c, se, nl, s in block_setting:\n",
    "            out_channels = int(c * width_mult)\n",
    "            stride = s if (dilation == 1) else 1\n",
    "            exp_channels = int(exp_size * width_mult)\n",
    "            layers.append(block(self.in_channels, out_channels, exp_channels, k, stride, dilation, se, nl, norm_layer))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "#         x = self.classifier(x)\n",
    "        flat1280 = self.flat1280(x)\n",
    "#         x = x.view(x.size(0), x.size(1))\n",
    "\n",
    "        lip_color = self.lip_color(flat1280)\n",
    "        eye_color = self.eye_color(flat1280)\n",
    "        hair = self.hair(flat1280)\n",
    "        hair_color = self.hair_color(flat1280)\n",
    "        gender = self.gender(flat1280)\n",
    "        earring = self.earring(flat1280)\n",
    "        smile = self.smile(flat1280)\n",
    "        frontal_face = self.frontal_face(flat1280)\n",
    "\n",
    "        lip_color = lip_color.view(lip_color.size(0), lip_color.size(1))\n",
    "        eye_color = eye_color.view(eye_color.size(0), eye_color.size(1))\n",
    "        hair = hair.view(hair.size(0), hair.size(1))\n",
    "        hair_color = hair_color.view(hair_color.size(0), hair_color.size(1))\n",
    "        gender = gender.view(gender.size(0), gender.size(1))\n",
    "        earring = earring.view(earring.size(0), earring.size(1))\n",
    "        smile = smile.view(smile.size(0), smile.size(1))\n",
    "        frontal_face = frontal_face.view(frontal_face.size(0), frontal_face.size(1))\n",
    "\n",
    "        hair = F.softmax(hair,dim = 1)\n",
    "        hair_color = F.softmax(hair_color,dim = 1)\n",
    "        gender = F.softmax(gender,dim = 1)\n",
    "        earring = F.softmax(earring,dim = 1)\n",
    "        smile = F.softmax(smile,dim = 1)\n",
    "        frontal_face = F.softmax(frontal_face,dim = 1)\n",
    "        \n",
    "        return [lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face]\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "def get_mobilenet_v3(mode='small', width_mult=1.0, pretrained=False, root='~/.torch/models', **kwargs):\n",
    "    model = MobileNetV3(mode=mode, width_mult=width_mult, **kwargs)\n",
    "    if pretrained:\n",
    "        raise ValueError(\"Not support pretrained\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_large_1_0(**kwargs):\n",
    "    return get_mobilenet_v3('large', 1.0, **kwargs)\n",
    "\n",
    "\n",
    "def mobilenet_v3_small_1_0(**kwargs):\n",
    "    return get_mobilenet_v3('small', 1.0, **kwargs)\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(s):\n",
    "        super().__init__()\n",
    "        s.MSE = torch.nn.MSELoss()\n",
    "        s.CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(s, preds, colors_b, attrs_b):\n",
    "#         y = a['lip_color']+a['eye_color']+[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]\n",
    "        lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = preds\n",
    "        lpc = s.MSE(lip_color, colors_b[:,:3])\n",
    "        lc = s.MSE(eye_color, colors_b[:,3:])\n",
    "        h = s.CE(hair, attrs_b[:, 0])\n",
    "        hc = s.CE(hair_color, attrs_b[:, 1])\n",
    "        g = s.CE(gender, attrs_b[:, 2])\n",
    "        e = s.CE(earring, attrs_b[:, 3])\n",
    "        sm = s.CE(smile, attrs_b[:, 4])\n",
    "        f = s.CE(frontal_face, attrs_b[:, 5])\n",
    "        loss = lpc+lc+h+hc+g+e+sm+f\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8e22db1"
   },
   "source": [
    "### 模型和损失函数说明\n",
    "模型使用Mobilenetv3的Small版本， 在将最后的输出层更改为8个并行的1x1卷积层，对2个颜色属性进行回归，对6个整型属性进行分类  \n",
    "回归损失使用MSE，分类损失使用交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a740a7b",
    "outputId": "28b1a3a2-3fbc-4c9a-f6d6-b3ae1513e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8925,  0.1118, -0.1723],\n",
       "         [-0.1632,  0.2536, -1.3730]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.0142, -0.2719,  0.8314],\n",
       "         [-2.5911, -0.9223, -0.0742]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.8087, -1.0785],\n",
       "         [ 0.4037,  1.3462]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.9686, -1.5095,  0.3642, -0.6491, -0.6247],\n",
       "         [-2.1093, -0.1983, -0.2731, -0.8785, -1.9321]], grad_fn=<ViewBackward>),\n",
       " tensor([[-2.1952,  0.4042],\n",
       "         [-2.5527, -0.1194]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.1786, -2.2074],\n",
       "         [ 0.2929, -1.5629]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.4470,  0.1229],\n",
       "         [-0.4061,  0.9815]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.8123, -1.0342],\n",
       "         [ 1.5224, -1.1291]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = mobilenet_v3_small_1_0(nclass=6)\n",
    "x = torch.rand((2, 3, 250, 250))\n",
    "y = m(x)\n",
    "for _ in y:\n",
    "    print(_.shape)\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = y\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653476255026,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "ae478728"
   },
   "outputs": [],
   "source": [
    "def save(savePath, m, epoch, acc):\n",
    "    d = {\n",
    "        'param': m.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'acc': acc,\n",
    "    }\n",
    "    if isinstance(savePath, Path):\n",
    "        savePath = savePath.as_posix()\n",
    "    torch.save(d, savePath)\n",
    "    print('checkpoint saved as', savePath)\n",
    "\n",
    "def load(loadPath):\n",
    "    if isinstance(loadPath, Path):\n",
    "        loadPath = loadPath.as_posix()\n",
    "    d = torch.load(loadPath)\n",
    "    m = Model(mode='small')\n",
    "    m.load_state_dict(d['param'])\n",
    "    print('checkpoint loaded from', loadPath)\n",
    "    e, acc = d['epoch'], d['acc']\n",
    "    print('epoch:', e, 'acc:', acc)\n",
    "    return m, d['epoch'], d['acc']\n",
    "\n",
    "def toCpu(path):\n",
    "    path = Path(path)\n",
    "    m, e, acc = load(path)\n",
    "    m.to(torch.device('cpu'))\n",
    "    save(path.parents[0]/f'{path.stem}_cpu.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1653476255452,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "9081a11d"
   },
   "outputs": [],
   "source": [
    "def val(em, param, val_dl, d):\n",
    "    l=Loss()\n",
    "    vn = len(val_dl.dataset)\n",
    "    em.load_state_dict(param)\n",
    "    hair_cnt = 0\n",
    "    hair_color_cnt = 0\n",
    "    gender_cnt = 0\n",
    "    earring_cnt = 0\n",
    "    smile_cnt = 0\n",
    "    frontal_face_cnt = 0\n",
    "    L = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(val_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            outs = em(xs)\n",
    "            lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = outs\n",
    "            L += l(outs, colors_b, attrs_b.long()).item()\n",
    "            hair_ = torch.max(hair, 1)[1]\n",
    "            hair_cnt += torch.sum(hair_ == attrs_b[:,0])\n",
    "            \n",
    "            hair_color_ = torch.max(hair_color, 1)[1]\n",
    "            hair_color_cnt += torch.sum(hair_color_ == attrs_b[:,1])\n",
    "            \n",
    "            gender_ = torch.max(gender, 1)[1]\n",
    "            gender_cnt += torch.sum(gender_ == attrs_b[:,2])\n",
    "            \n",
    "            earring_ = torch.max(earring, 1)[1]\n",
    "            earring_cnt += torch.sum(earring_ == attrs_b[:,3])\n",
    "            \n",
    "            smile_ = torch.max(smile, 1)[1]\n",
    "            smile_cnt += torch.sum(smile_ == attrs_b[:,4])\n",
    "            \n",
    "            frontal_face_ = torch.max(frontal_face, 1)[1]\n",
    "            frontal_face_cnt += torch.sum(frontal_face_ == attrs_b[:,5])\n",
    "    acc = (hair_cnt+hair_color_cnt+gender_cnt+earring_cnt+smile_cnt+frontal_face_cnt)/6/vn\n",
    "    print(f'validated on {vn} samples| mean acc:{acc*100:.4f}%')\n",
    "    print(f'hair_cnt:{hair_cnt/vn}|hair_color_cnt:{hair_color_cnt/vn}|gender_cnt:{gender_cnt/vn}')\n",
    "    print(f'earring_cnt:{earring_cnt/vn}|smile_cnt:{smile_cnt/vn}|frontal_face_cnt:{frontal_face_cnt/vn}')\n",
    "    print(f'loss:{L/vn:.4f}')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train(m,\n",
    "          d,\n",
    "          train_dl,\n",
    "          val_dl,\n",
    "          saveDir=Path('save'),\n",
    "          resumePath=None,\n",
    "          lr=0.001,\n",
    "          e=50,\n",
    "          s=10\n",
    "         ):\n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    startEp = -1\n",
    "    b = 0\n",
    "    try:\n",
    "        m, startEp, b = load(saveDir/'best.ckpt')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    if resumePath is not None:\n",
    "        m, startEp, b = load(resumePath)\n",
    "\n",
    "    m.to(d).train()\n",
    "    em = Model(mode = 'small').to(d).eval()\n",
    "    \n",
    "    l=Loss()\n",
    "    o=torch.optim.SGD(m.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    tn = len(train_dl.dataset)\n",
    "    t = tqdm(range(startEp+1, e))\n",
    "#     t = range(startEp+1, e)\n",
    "    for ep in t:\n",
    "        L = 0\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(train_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            o.zero_grad()\n",
    "            outs = m(xs)\n",
    "            loss = l(outs, colors_b, attrs_b.long())\n",
    "            loss.backward()\n",
    "            o.step()\n",
    "\n",
    "            L += loss.item()\n",
    "        t.set_description(f'ep:{ep}| L:{L/tn:.6f}')\n",
    "        if (ep+1)%s != 0: continue\n",
    "\n",
    "        acc = val(em, m.state_dict(), val_dl, d)\n",
    "        save(saveDir/f'{ep:05d}_{acc:.4f}.ckpt', m, ep, acc)\n",
    "        if b < acc:\n",
    "            b = acc\n",
    "            save(saveDir/'best.ckpt', m, ep, acc)\n",
    "        print(f'E:{ep}| L:{L/tn:.6f}')\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6e5fcfb"
   },
   "outputs": [],
   "source": [
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(d)\n",
    "# d = torch.device('cpu')\n",
    "Model = MobileNetV3\n",
    "m = Model(mode = 'small').to(d)\n",
    "train(m,\n",
    "      d,\n",
    "      train_dl,\n",
    "      val_dl,\n",
    "      saveDir=Path(rootdir + 'save'),\n",
    "#       resumePath=Path('save2/03999_0.8062.ckpt'),\n",
    "      lr=0.0001,\n",
    "      e=400,\n",
    "      s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238574,
     "status": "ok",
     "timestamp": 1653476538736,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "06aa49de",
    "outputId": "dbcb9008-a984-4f52-883f-1062438623e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded from /content/drive/MyDrive/DeepLearningHW/save/best.ckpt\n",
      "epoch: 399 acc: tensor(0.7439, device='cuda:0')\n",
      "validated on 1046 samples| mean acc:74.3945%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4015296399593353|gender_cnt:0.8116634488105774\n",
      "earring_cnt:0.8250477910041809|smile_cnt:0.6414914131164551|frontal_face_cnt:0.8336520195007324\n",
      "loss:32.6045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7439, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Model = MobileNetV3\n",
    "em, epo, acc = load(rootdir + 'save/best.ckpt')\n",
    "val(Model(mode='small').to(d).eval(), em.state_dict(), val_dl, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result\n",
    "epoch: 399 acc: tensor(0.7439, device='cuda:0')  \n",
    "validated on 1046 samples| mean acc:74.3945%  \n",
    "  \n",
    "hair_cnt:0.9502868056297302  \n",
    "hair_color_cnt:0.4015296399593353  \n",
    "gender_cnt:0.8116634488105774   \n",
    "earring_cnt:0.8250477910041809  \n",
    "smile_cnt:0.6414914131164551  \n",
    "frontal_face_cnt:0.8336520195007324  \n",
    "loss:32.6045  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FS2K.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
