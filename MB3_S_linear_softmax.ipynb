{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1653476251619,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "f1983b99",
    "outputId": "72c63301-a80a-4fd9-d902-44f645ff2eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2, json, numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5199c1c3"
   },
   "source": [
    "### 路径说明\n",
    "```\n",
    "FS2K\n",
    "├─data # FS2K数据集位置\n",
    "│  └─FS2K\n",
    "│      ├─photo\n",
    "│      │  ├─photo1\n",
    "│      │  ├─photo2\n",
    "│      │  └─photo3\n",
    "│      └─sketch\n",
    "│          ├─sketch1\n",
    "│          ├─sketch2\n",
    "│          └─sketch3\n",
    "├─save # 模型保存位置\n",
    "└─FS2K.ipynb # 代码\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1653476252735,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "bdac8441"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义Dataset\n",
    "class DS(Dataset):\n",
    "    def __init__(s, dataD, mode='train'):\n",
    "        super().__init__()\n",
    "        s.dataD = dataD\n",
    "        s.mode = mode\n",
    "        s.xtf = transforms.Compose([\n",
    "            transforms.Resize((250,250)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        s.ytf = transforms.Compose([\n",
    "            torch.tensor,\n",
    "        ])\n",
    "        s.data = s.read()\n",
    "    \n",
    "    def read(s):\n",
    "        D = s.dataD\n",
    "        jp = D / f'anno_{s.mode}.json'\n",
    "        with jp.open('r', encoding='utf-8')as f:\n",
    "            annos = json.load(f)\n",
    "        return annos\n",
    "\n",
    "    def __getitem__(s, i):\n",
    "        a = s.data[i]\n",
    "        imgP = s.dataD/ f\"photo/{a['image_name']}.jpg\"\n",
    "        img = s.xtf(Image.open(imgP.as_posix()))\n",
    "        colors = a['lip_color']+a['eye_color']\n",
    "        attrs = list(map(int,[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]))\n",
    "        return img, s.ytf(colors), torch.tensor(attrs, dtype=int)\n",
    "\n",
    "    def __len__(s):\n",
    "        return len(s.data)\n",
    "\n",
    "rootdir = '/content/drive/MyDrive/DeepLearningHW/'\n",
    "# 实例化Dataset\n",
    "dataD = Path(rootdir + 'data/FS2K')\n",
    "train_ds = DS(dataD)\n",
    "val_ds = DS(dataD, 'test')\n",
    "\n",
    "# 创建Dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaf59a07"
   },
   "source": [
    "### Dataset说明\n",
    "每一个样本包含三个变量img, colors, attrs  \n",
    "img为tensor图片  \n",
    "colors为一个6元素的float类型一维数组, 前三个表示嘴唇颜色lip_color, 后三个表示眼睛颜色eye_color  \n",
    "attrs为6元素的整型一位数组, 分别为hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d058427f",
    "outputId": "fabb2c80-7bc0-4c7f-a3ba-21be81680ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([156.9775,  82.5112,  79.0000, 118.6518,  72.2589,  69.5982]),\n",
       " tensor([0, 2, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, colors, attrs = train_ds[0]\n",
    "colors, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb335b7f",
    "outputId": "62354c27-3732-4d18-c8f3-72a3bef8732a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'photo1/image0110',\n",
       " 'skin_patch': [163, 139],\n",
       " 'lip_color': [156.97750511247443, 82.51124744376278, 79.0],\n",
       " 'eye_color': [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
       " 'hair': 0,\n",
       " 'hair_color': 2,\n",
       " 'gender': 0,\n",
       " 'earring': 1,\n",
       " 'smile': 1,\n",
       " 'frontal_face': 1,\n",
       " 'style': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"image_name\": \"photo1/image0110\",\n",
    "\n",
    "    \"skin_patch\": [163, 139],\n",
    "    # a point of face region.\n",
    "\n",
    "    \"lip_color\": [156.97750511247443, 82.51124744376278, 79.0],\n",
    "    # the mean RGB value of lip area.\n",
    "\n",
    "    \"eye_color\": [118.65178571428571, 72.25892857142857, 69.59821428571429],\n",
    "    # the mean RGB value of eye area.\n",
    "\n",
    "    \"hair\": 0,\n",
    "    # 0: with hair, 1: without hair.\n",
    "\n",
    "    \"hair_color\": 2,\n",
    "    # 0: brown, 1: black, 2: red, 3: no-hair, 4: golden.\n",
    "\n",
    "    \"gender\": 0,\n",
    "    # 0: male, 1: female.\n",
    "\n",
    "    \"earring\": 1,\n",
    "    # 0: with earring, 1: without earring.\n",
    "\n",
    "    \"smile\": 1,\n",
    "    # 0: with smile, 1: without smile.\n",
    "\n",
    "    \"frontal_face\": 1,\n",
    "    # 0: head rotates within 30 degrees, 1: > 30 degrees\n",
    "\n",
    "    \"style\": 0\n",
    "    # Style = one of {0, 1, 2}, please refer to the sketch samples.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1653476702365,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "9561edc6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class _Hswish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(_Hswish, self).__init__()\n",
    "        self.relu6 = nn.ReLU6(inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class _Hsigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(_Hsigmoid, self).__init__()\n",
    "        self.relu6 = nn.ReLU6(inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu6(x + 3.) / 6.\n",
    "\n",
    "\n",
    "class _ConvBNHswish(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(_ConvBNHswish, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = _Hswish(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            _Hsigmoid(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, _, _ = x.size()\n",
    "        out = self.avg_pool(x).view(n, c)\n",
    "        out = self.fc(out).view(n, c, 1, 1)\n",
    "        return x * out.expand_as(x)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, exp_size, kernel_size, stride, dilation=1, se=False, nl='RE',\n",
    "                 norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "        self.use_res_connect = stride == 1 and in_channels == out_channels\n",
    "        if nl == 'HS':\n",
    "            act = _Hswish\n",
    "        else:\n",
    "            act = nn.ReLU\n",
    "        if se:\n",
    "            SELayer = SEModule\n",
    "        else:\n",
    "            SELayer = Identity\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            nn.Conv2d(in_channels, exp_size, 1, bias=False),\n",
    "            norm_layer(exp_size),\n",
    "            act(True),\n",
    "            # dw\n",
    "            nn.Conv2d(exp_size, exp_size, kernel_size, stride, (kernel_size - 1) // 2 * dilation,\n",
    "                      dilation, groups=exp_size, bias=False),\n",
    "            norm_layer(exp_size),\n",
    "            SELayer(exp_size),\n",
    "            act(True),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(exp_size, out_channels, 1, bias=False),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, nclass=1000, mode='large', width_mult=1.0, dilated=False, norm_layer=nn.BatchNorm2d):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        if mode == 'large':\n",
    "            layer1_setting = [\n",
    "                # k, exp_size, c, se, nl, s\n",
    "                [3, 16, 16, False, 'RE', 1],\n",
    "                [3, 64, 24, False, 'RE', 2],\n",
    "                [3, 72, 24, False, 'RE', 1], ]\n",
    "            layer2_setting = [\n",
    "                [5, 72, 40, True, 'RE', 2],\n",
    "                [5, 120, 40, True, 'RE', 1],\n",
    "                [5, 120, 40, True, 'RE', 1], ]\n",
    "            layer3_setting = [\n",
    "                [3, 240, 80, False, 'HS', 2],\n",
    "                [3, 200, 80, False, 'HS', 1],\n",
    "                [3, 184, 80, False, 'HS', 1],\n",
    "                [3, 184, 80, False, 'HS', 1],\n",
    "                [3, 480, 112, True, 'HS', 1],\n",
    "                [3, 672, 112, True, 'HS', 1],\n",
    "                [5, 672, 112, True, 'HS', 1], ]\n",
    "            layer4_setting = [\n",
    "                [5, 672, 160, True, 'HS', 2],\n",
    "                [5, 960, 160, True, 'HS', 1], ]\n",
    "        elif mode == 'small':\n",
    "            layer1_setting = [\n",
    "                # k, exp_size, c, se, nl, s\n",
    "                [3, 16, 16, True, 'RE', 2], ]\n",
    "            layer2_setting = [\n",
    "                [3, 72, 24, False, 'RE', 2],\n",
    "                [3, 88, 24, False, 'RE', 1], ]\n",
    "            layer3_setting = [\n",
    "                [5, 96, 40, True, 'HS', 2],\n",
    "                [5, 240, 40, True, 'HS', 1],\n",
    "                [5, 240, 40, True, 'HS', 1],\n",
    "                [5, 120, 48, True, 'HS', 1],\n",
    "                [5, 144, 48, True, 'HS', 1], ]\n",
    "            layer4_setting = [\n",
    "                [5, 288, 96, True, 'HS', 2],\n",
    "                [5, 576, 96, True, 'HS', 1],\n",
    "                [5, 576, 96, True, 'HS', 1], ]\n",
    "        else:\n",
    "            raise ValueError('Unknown mode.')\n",
    "\n",
    "        # building first layer\n",
    "        self.in_channels = int(16 * width_mult) if width_mult > 1.0 else 16\n",
    "        self.conv1 = _ConvBNHswish(3, self.in_channels, 3, 2, 1, norm_layer=norm_layer)\n",
    "\n",
    "        # building bottleneck blocks\n",
    "        self.layer1 = self._make_layer(Bottleneck, layer1_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        self.layer2 = self._make_layer(Bottleneck, layer2_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        self.layer3 = self._make_layer(Bottleneck, layer3_setting,\n",
    "                                       width_mult, norm_layer=norm_layer)\n",
    "        if dilated:\n",
    "            self.layer4 = self._make_layer(Bottleneck, layer4_setting,\n",
    "                                           width_mult, dilation=2, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.layer4 = self._make_layer(Bottleneck, layer4_setting,\n",
    "                                           width_mult, norm_layer=norm_layer)\n",
    "\n",
    "        # building last several layers\n",
    "        classifier = list()\n",
    "        if mode == 'large':\n",
    "            last_bneck_channels = int(960 * width_mult) if width_mult > 1.0 else 960\n",
    "            self.layer5 = _ConvBNHswish(self.in_channels, last_bneck_channels, 1, norm_layer=norm_layer)\n",
    "            classifier.append(nn.AdaptiveAvgPool2d(1))\n",
    "            classifier.append(nn.Conv2d(last_bneck_channels, 1280, 1))\n",
    "            classifier.append(_Hswish(True))\n",
    "#             classifier.append(nn.Conv2d(1280, nclass, 1))\n",
    "        elif mode == 'small':\n",
    "            last_bneck_channels = int(576 * width_mult) if width_mult > 1.0 else 576\n",
    "            self.layer5 = _ConvBNHswish(self.in_channels, last_bneck_channels, 1, norm_layer=norm_layer)\n",
    "            classifier.append(SEModule(last_bneck_channels))\n",
    "            classifier.append(nn.AdaptiveAvgPool2d(1))\n",
    "            classifier.append(nn.Conv2d(last_bneck_channels, 1280, 1))\n",
    "            classifier.append(_Hswish(True))\n",
    "#             classifier.append(nn.Conv2d(1280, nclass, 1))\n",
    "        else:\n",
    "            raise ValueError('Unknown mode.')\n",
    "#         self.classifier = nn.Sequential(*classifier)\n",
    "\n",
    "        self.flat1280 = nn.Sequential(*classifier)\n",
    "        n = 1280\n",
    "\n",
    "        self.lip_color = nn.Linear(n, 3)\n",
    "        self.eye_color = nn.Linear(n, 3)\n",
    "        self.hair = nn.Linear(n, 2)\n",
    "        self.hair_color = nn.Linear(n, 5)\n",
    "        self.gender = nn.Linear(n, 2)\n",
    "        self.earring = nn.Linear(n, 2)\n",
    "        self.smile = nn.Linear(n, 2)\n",
    "        self.frontal_face = nn.Linear(n, 2)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, block_setting, width_mult, dilation=1, norm_layer=nn.BatchNorm2d):\n",
    "        layers = list()\n",
    "        for k, exp_size, c, se, nl, s in block_setting:\n",
    "            out_channels = int(c * width_mult)\n",
    "            stride = s if (dilation == 1) else 1\n",
    "            exp_channels = int(exp_size * width_mult)\n",
    "            layers.append(block(self.in_channels, out_channels, exp_channels, k, stride, dilation, se, nl, norm_layer))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "#         x = self.classifier(x)\n",
    "        flat1280 = self.flat1280(x)\n",
    "#         x = x.view(x.size(0), x.size(1))\n",
    "        \n",
    "        flat1280 = flat1280.view(flat1280.size(0),flat1280.size(1))\n",
    "\n",
    "        lip_color = self.lip_color(flat1280)\n",
    "        eye_color = self.eye_color(flat1280)\n",
    "        hair = self.hair(flat1280)\n",
    "        hair_color = self.hair_color(flat1280)\n",
    "        gender = self.gender(flat1280)\n",
    "        earring = self.earring(flat1280)\n",
    "        smile = self.smile(flat1280)\n",
    "        frontal_face = self.frontal_face(flat1280)\n",
    "\n",
    "        hair = F.softmax(hair,dim = 1)\n",
    "        hair_color = F.softmax(hair_color,dim = 1)\n",
    "        gender = F.softmax(gender,dim = 1)\n",
    "        earring = F.softmax(earring,dim = 1)\n",
    "        smile = F.softmax(smile,dim = 1)\n",
    "        frontal_face = F.softmax(frontal_face,dim = 1)\n",
    "        \n",
    "        return [lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face]\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "def get_mobilenet_v3(mode='small', width_mult=1.0, pretrained=False, root='~/.torch/models', **kwargs):\n",
    "    model = MobileNetV3(mode=mode, width_mult=width_mult, **kwargs)\n",
    "    if pretrained:\n",
    "        raise ValueError(\"Not support pretrained\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_large_1_0(**kwargs):\n",
    "    return get_mobilenet_v3('large', 1.0, **kwargs)\n",
    "\n",
    "\n",
    "def mobilenet_v3_small_1_0(**kwargs):\n",
    "    return get_mobilenet_v3('small', 1.0, **kwargs)\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(s):\n",
    "        super().__init__()\n",
    "        s.MSE = torch.nn.MSELoss()\n",
    "        s.CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(s, preds, colors_b, attrs_b):\n",
    "#         y = a['lip_color']+a['eye_color']+[a['hair'],a['hair_color'],a['gender'],a['earring'],a['smile'],a['frontal_face']]\n",
    "        lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = preds\n",
    "        lpc = s.MSE(lip_color, colors_b[:,:3])\n",
    "        lc = s.MSE(eye_color, colors_b[:,3:])\n",
    "        h = s.CE(hair, attrs_b[:, 0])\n",
    "        hc = s.CE(hair_color, attrs_b[:, 1])\n",
    "        g = s.CE(gender, attrs_b[:, 2])\n",
    "        e = s.CE(earring, attrs_b[:, 3])\n",
    "        sm = s.CE(smile, attrs_b[:, 4])\n",
    "        f = s.CE(frontal_face, attrs_b[:, 5])\n",
    "        loss = lpc+lc+h+hc+g+e+sm+f\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8e22db1"
   },
   "source": [
    "### 模型和损失函数说明\n",
    "模型使用Mobilenetv3的Small版本， 在将最后的输出层更改为8个并行的1x1卷积层，对2个颜色属性进行回归，对6个整型属性进行分类  \n",
    "回归损失使用MSE，分类损失使用交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a740a7b",
    "outputId": "28b1a3a2-3fbc-4c9a-f6d6-b3ae1513e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8925,  0.1118, -0.1723],\n",
       "         [-0.1632,  0.2536, -1.3730]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.0142, -0.2719,  0.8314],\n",
       "         [-2.5911, -0.9223, -0.0742]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.8087, -1.0785],\n",
       "         [ 0.4037,  1.3462]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.9686, -1.5095,  0.3642, -0.6491, -0.6247],\n",
       "         [-2.1093, -0.1983, -0.2731, -0.8785, -1.9321]], grad_fn=<ViewBackward>),\n",
       " tensor([[-2.1952,  0.4042],\n",
       "         [-2.5527, -0.1194]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.1786, -2.2074],\n",
       "         [ 0.2929, -1.5629]], grad_fn=<ViewBackward>),\n",
       " tensor([[-0.4470,  0.1229],\n",
       "         [-0.4061,  0.9815]], grad_fn=<ViewBackward>),\n",
       " tensor([[ 0.8123, -1.0342],\n",
       "         [ 1.5224, -1.1291]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = mobilenet_v3_small_1_0(nclass=6)\n",
    "x = torch.rand((2, 3, 250, 250))\n",
    "y = m(x)\n",
    "for _ in y:\n",
    "    print(_.shape)\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = y\n",
    "lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653476255026,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "ae478728"
   },
   "outputs": [],
   "source": [
    "def save(savePath, m, epoch, acc):\n",
    "    d = {\n",
    "        'param': m.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'acc': acc,\n",
    "    }\n",
    "    if isinstance(savePath, Path):\n",
    "        savePath = savePath.as_posix()\n",
    "    torch.save(d, savePath)\n",
    "    print('checkpoint saved as', savePath)\n",
    "\n",
    "def load(loadPath):\n",
    "    if isinstance(loadPath, Path):\n",
    "        loadPath = loadPath.as_posix()\n",
    "    d = torch.load(loadPath)\n",
    "    m = Model(mode='small')\n",
    "    m.load_state_dict(d['param'])\n",
    "    print('checkpoint loaded from', loadPath)\n",
    "    e, acc = d['epoch'], d['acc']\n",
    "    print('epoch:', e, 'acc:', acc)\n",
    "    return m, d['epoch'], d['acc']\n",
    "\n",
    "def toCpu(path):\n",
    "    path = Path(path)\n",
    "    m, e, acc = load(path)\n",
    "    m.to(torch.device('cpu'))\n",
    "    save(path.parents[0]/f'{path.stem}_cpu.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1653476255452,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "9081a11d"
   },
   "outputs": [],
   "source": [
    "def val(em, param, val_dl, d):\n",
    "    l=Loss()\n",
    "    vn = len(val_dl.dataset)\n",
    "    em.load_state_dict(param)\n",
    "    hair_cnt = 0\n",
    "    hair_color_cnt = 0\n",
    "    gender_cnt = 0\n",
    "    earring_cnt = 0\n",
    "    smile_cnt = 0\n",
    "    frontal_face_cnt = 0\n",
    "    L = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(val_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            outs = em(xs)\n",
    "            lip_color, eye_color, hair, hair_color, gender, earring, smile, frontal_face = outs\n",
    "            L += l(outs, colors_b, attrs_b.long()).item()\n",
    "            hair_ = torch.max(hair, 1)[1]\n",
    "            hair_cnt += torch.sum(hair_ == attrs_b[:,0])\n",
    "            \n",
    "            hair_color_ = torch.max(hair_color, 1)[1]\n",
    "            hair_color_cnt += torch.sum(hair_color_ == attrs_b[:,1])\n",
    "            \n",
    "            gender_ = torch.max(gender, 1)[1]\n",
    "            gender_cnt += torch.sum(gender_ == attrs_b[:,2])\n",
    "            \n",
    "            earring_ = torch.max(earring, 1)[1]\n",
    "            earring_cnt += torch.sum(earring_ == attrs_b[:,3])\n",
    "            \n",
    "            smile_ = torch.max(smile, 1)[1]\n",
    "            smile_cnt += torch.sum(smile_ == attrs_b[:,4])\n",
    "            \n",
    "            frontal_face_ = torch.max(frontal_face, 1)[1]\n",
    "            frontal_face_cnt += torch.sum(frontal_face_ == attrs_b[:,5])\n",
    "    acc = (hair_cnt+hair_color_cnt+gender_cnt+earring_cnt+smile_cnt+frontal_face_cnt)/6/vn\n",
    "    print(f'validated on {vn} samples| mean acc:{acc*100:.4f}%')\n",
    "    print(f'hair_cnt:{hair_cnt/vn}|hair_color_cnt:{hair_color_cnt/vn}|gender_cnt:{gender_cnt/vn}')\n",
    "    print(f'earring_cnt:{earring_cnt/vn}|smile_cnt:{smile_cnt/vn}|frontal_face_cnt:{frontal_face_cnt/vn}')\n",
    "    print(f'loss:{L/vn:.4f}')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train(m,\n",
    "          d,\n",
    "          train_dl,\n",
    "          val_dl,\n",
    "          saveDir=Path('save'),\n",
    "          resumePath=None,\n",
    "          lr=0.001,\n",
    "          e=50,\n",
    "          s=10\n",
    "         ):\n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    startEp = -1\n",
    "    b = 0\n",
    "    try:\n",
    "        m, startEp, b = load(saveDir/'best.ckpt')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    if resumePath is not None:\n",
    "        m, startEp, b = load(resumePath)\n",
    "\n",
    "    m.to(d).train()\n",
    "    em = Model(mode = 'small').to(d).eval()\n",
    "    \n",
    "    l=Loss()\n",
    "    o=torch.optim.SGD(m.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    saveDir.mkdir(exist_ok=1)\n",
    "    tn = len(train_dl.dataset)\n",
    "    t = tqdm(range(startEp+1, e))\n",
    "#     t = range(startEp+1, e)\n",
    "    for ep in t:\n",
    "        L = 0\n",
    "        for i, (xs, colors_b, attrs_b) in enumerate(train_dl):\n",
    "            xs, colors_b, attrs_b = xs.to(d), colors_b.to(d), attrs_b.to(d)\n",
    "            o.zero_grad()\n",
    "            outs = m(xs)\n",
    "            loss = l(outs, colors_b, attrs_b.long())\n",
    "            loss.backward()\n",
    "            o.step()\n",
    "\n",
    "            L += loss.item()\n",
    "        t.set_description(f'ep:{ep}| L:{L/tn:.6f}')\n",
    "        if (ep+1)%s != 0: continue\n",
    "\n",
    "        acc = val(em, m.state_dict(), val_dl, d)\n",
    "        save(saveDir/f'{ep:05d}_{acc:.4f}.ckpt', m, ep, acc)\n",
    "        if b < acc:\n",
    "            b = acc\n",
    "            save(saveDir/'best.ckpt', m, ep, acc)\n",
    "        print(f'E:{ep}| L:{L/tn:.6f}')\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3278052,
     "status": "ok",
     "timestamp": 1653480051545,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "e6e5fcfb",
    "outputId": "0c515e32-769c-41ad-b6f8-a36ccd6bb5c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLearningHW/save2/best.ckpt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:19| L:49.853137:   5%|▌         | 20/400 [06:13<55:42,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:70.2836%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39483746886253357|gender_cnt:0.576481819152832\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6405353546142578|frontal_face_cnt:0.8336520195007324\n",
      "loss:48.2192\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00019_0.7028.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:19| L:49.853137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:39| L:35.990895:  10%|█         | 40/400 [08:43<51:50,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:70.3952%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3929254412651062|gender_cnt:0.5783938765525818\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6472275257110596|frontal_face_cnt:0.8336520195007324\n",
      "loss:40.6936\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00039_0.7040.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:39| L:35.990895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:59| L:24.055860:  15%|█▌        | 60/400 [11:14<50:07,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:69.9171%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39961758255958557|gender_cnt:0.5535373091697693\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6367112994194031|frontal_face_cnt:0.8336520195007324\n",
      "loss:34.3706\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00059_0.6992.ckpt\n",
      "E:59| L:24.055860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:79| L:24.567601:  20%|██        | 80/400 [13:45<47:09,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:70.2836%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39483746886253357|gender_cnt:0.5755258202552795\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6414914131164551|frontal_face_cnt:0.8336520195007324\n",
      "loss:35.7280\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00079_0.7028.ckpt\n",
      "E:79| L:24.567601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:99| L:15.582996:  25%|██▌       | 100/400 [16:16<43:51,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:72.0045%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39579349756240845|gender_cnt:0.6606118679046631\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6586998105049133|frontal_face_cnt:0.8336520195007324\n",
      "loss:32.4120\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00099_0.7200.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:99| L:15.582996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:119| L:10.873522:  30%|███       | 120/400 [18:47<40:19,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.7094%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3967495262622833|gender_cnt:0.7562141418457031\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6644359230995178|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.7667\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00119_0.7371.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:119| L:10.873522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:139| L:9.098314:  35%|███▌      | 140/400 [21:16<37:16,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.3748%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3977055549621582|gender_cnt:0.7495219707489014\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6500955820083618|frontal_face_cnt:0.8336520195007324\n",
      "loss:32.6309\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00139_0.7337.ckpt\n",
      "E:139| L:9.098314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:159| L:10.716664:  40%|████      | 160/400 [23:46<35:34,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.7890%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3977055549621582|gender_cnt:0.7638623118400574\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6606118679046631|frontal_face_cnt:0.8336520195007324\n",
      "loss:29.4684\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00159_0.7379.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:159| L:10.716664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:179| L:9.535363:  45%|████▌     | 180/400 [26:18<32:12,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.1198%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4015296399593353|gender_cnt:0.7304015159606934\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6500955820083618|frontal_face_cnt:0.8336520195007324\n",
      "loss:31.3580\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00179_0.7312.ckpt\n",
      "E:179| L:9.535363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:199| L:10.281130:  50%|█████     | 200/400 [28:48<28:55,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.4066%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4072657823562622|gender_cnt:0.7476099133491516\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6443594694137573|frontal_face_cnt:0.8336520195007324\n",
      "loss:29.4222\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00199_0.7341.ckpt\n",
      "E:199| L:10.281130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:219| L:6.141005:  55%|█████▌    | 220/400 [31:19<26:21,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.6616%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.40535372495651245|gender_cnt:0.7619502544403076\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6472275257110596|frontal_face_cnt:0.8336520195007324\n",
      "loss:29.5219\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00219_0.7366.ckpt\n",
      "E:219| L:6.141005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:239| L:5.456790:  60%|██████    | 240/400 [33:54<23:54,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.7572%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4015296399593353|gender_cnt:0.7667303681373596\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6520076394081116|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.6603\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00239_0.7376.ckpt\n",
      "E:239| L:5.456790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:259| L:5.712345:  65%|██████▌   | 260/400 [36:28<20:39,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.6934%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4024856686592102|gender_cnt:0.7705544829368591\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6434034109115601|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.7866\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00259_0.7369.ckpt\n",
      "E:259| L:5.712345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:279| L:6.922174:  70%|███████   | 280/400 [39:04<17:58,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:74.0121%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39483746886253357|gender_cnt:0.7868068814277649\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6539196968078613|frontal_face_cnt:0.8336520195007324\n",
      "loss:29.0562\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00279_0.7401.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:279| L:6.922174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:299| L:5.604718:  75%|███████▌  | 300/400 [41:39<15:02,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.9006%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.40057361125946045|gender_cnt:0.7753345966339111\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6529636383056641|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.8670\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00299_0.7390.ckpt\n",
      "E:299| L:5.604718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:319| L:5.780342:  80%|████████  | 320/400 [44:14<11:58,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:74.1236%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39961758255958557|gender_cnt:0.7801147103309631\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6625239253044128|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.8141\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00319_0.7412.ckpt\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "E:319| L:5.780342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:339| L:4.616464:  85%|████████▌ | 340/400 [46:50<09:06,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.6934%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3986615538597107|gender_cnt:0.7648183703422546\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6529636383056641|frontal_face_cnt:0.8336520195007324\n",
      "loss:30.3318\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00339_0.7369.ckpt\n",
      "E:339| L:4.616464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:359| L:4.053710:  90%|█████████ | 360/400 [49:25<05:57,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.8209%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.4034416675567627|gender_cnt:0.7715104818344116\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6491395831108093|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.0654\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00359_0.7382.ckpt\n",
      "E:359| L:4.053710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:379| L:5.657294:  95%|█████████▌| 380/400 [52:01<03:01,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.8368%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.3967495262622833|gender_cnt:0.7619502544403076\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6663479804992676|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.6951\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00379_0.7384.ckpt\n",
      "E:379| L:5.657294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep:399| L:5.222742: 100%|██████████| 400/400 [54:36<00:00,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validated on 1046 samples| mean acc:73.9165%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.41108986735343933|gender_cnt:0.7782026529312134\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6405353546142578|frontal_face_cnt:0.8336520195007324\n",
      "loss:29.4963\n",
      "checkpoint saved as /content/drive/MyDrive/DeepLearningHW/save2/00399_0.7392.ckpt\n",
      "E:399| L:5.222742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(d)\n",
    "# d = torch.device('cpu')\n",
    "Model = MobileNetV3\n",
    "m = Model(mode = 'small').to(d)\n",
    "train(m,\n",
    "      d,\n",
    "      train_dl,\n",
    "      val_dl,\n",
    "      saveDir=Path(rootdir + 'save2'),\n",
    "#       resumePath=Path('save2/03999_0.8062.ckpt'),\n",
    "      lr=0.0001,\n",
    "      e=400,\n",
    "      s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5491,
     "status": "ok",
     "timestamp": 1653480357138,
     "user": {
      "displayName": "JL D",
      "userId": "12622209343116587532"
     },
     "user_tz": -480
    },
    "id": "06aa49de",
    "outputId": "c349c229-e0d7-48d5-a18a-19ce0cb70c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded from /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt\n",
      "epoch: 319 acc: tensor(0.7412, device='cuda:0')\n",
      "validated on 1046 samples| mean acc:74.1236%\n",
      "hair_cnt:0.9502868056297302|hair_color_cnt:0.39961758255958557|gender_cnt:0.7801147103309631\n",
      "earring_cnt:0.8212236762046814|smile_cnt:0.6625239253044128|frontal_face_cnt:0.8336520195007324\n",
      "loss:28.7963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7412, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Model = MobileNetV3\n",
    "em, epo, acc = load(rootdir + 'save2/best.ckpt')\n",
    "val(Model(mode='small').to(d).eval(), em.state_dict(), val_dl, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result\n",
    "checkpoint loaded from /content/drive/MyDrive/DeepLearningHW/save2/best.ckpt  \n",
    "epoch: 319 acc: tensor(0.7412, device='cuda:0')  \n",
    "validated on 1046 samples| mean acc:74.1236%  \n",
    "  \n",
    "hair_cnt:0.9502868056297302  \n",
    "hair_color_cnt:0.39961758255958557  \n",
    "gender_cnt:0.7801147103309631  \n",
    "earring_cnt:0.8212236762046814  \n",
    "smile_cnt:0.6625239253044128  \n",
    "frontal_face_cnt:0.8336520195007324  \n",
    "loss:28.7963  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FS2K.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
